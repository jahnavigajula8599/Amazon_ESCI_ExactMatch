{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86bd2213-0ad2-4847-8a33-397fd9ebb065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data: 2,621,288 examples, 1,814,924 products\n",
      "Label distribution:\n",
      "esci_label\n",
      "E    1708158\n",
      "S     574313\n",
      "I     263165\n",
      "C      75652\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After filtering: 1,247,558 rows\n",
      "Unique queries: 97,344\n",
      "Unique products: 904,348\n",
      "\n",
      "Products per query: mean=12.8, median=14\n",
      "Min=1, Max=146\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Data Loading & Initial Exploration\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Goal: Understand the data structure and sampling requirements\n",
    "- Filter US locale + Exact matches only\n",
    "- Sample ~50 queries, ~500 rows total\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "examples = pd.read_parquet(\"shopping_queries_dataset_examples.parquet\", engine='fastparquet')\n",
    "products = pd.read_parquet(\"shopping_queries_dataset_products.parquet\", engine='fastparquet')\n",
    "\n",
    "print(f\"Raw data: {len(examples):,} examples, {len(products):,} products\")\n",
    "print(f\"Label distribution:\\n{examples['esci_label'].value_counts()}\\n\")\n",
    "\n",
    "# Filter per requirements: US + Exact matches\n",
    "filtered = examples[\n",
    "    (examples['product_locale'] == 'us') & \n",
    "    (examples['esci_label'] == 'E')\n",
    "].copy()\n",
    "\n",
    "print(f\"After filtering: {len(filtered):,} rows\")\n",
    "print(f\"Unique queries: {filtered['query_id'].nunique():,}\")\n",
    "print(f\"Unique products: {filtered['product_id'].nunique():,}\")\n",
    "\n",
    "# Check products per query distribution\n",
    "ppq = filtered.groupby('query_id').size()\n",
    "print(f\"\\nProducts per query: mean={ppq.mean():.1f}, median={ppq.median():.0f}\")\n",
    "print(f\"Min={ppq.min()}, Max={ppq.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32f8f75-0d12-424f-8cc0-4370cbce2cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with sampled queries: 692\n",
      "\n",
      "Final sample:\n",
      "  Rows: 500\n",
      "  Queries: 49\n",
      "  Products: 499\n",
      "\n",
      "Sample queries:\n",
      "  - pickled white wood stain\n",
      "  - women's birthday gifts\n",
      "  - iblason phone case\n",
      "  - magnolia market\n",
      "  - lightweight messenger bag for women\n",
      "\n",
      " Saved to sample_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Sampling Strategy\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Decision: Sample 50 queries first, then take 500 rows from those queries\n",
    "Reasoning: Ensures diverse queries while meeting row count requirement\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample queries\n",
    "query_ids = filtered['query_id'].unique()\n",
    "sampled_qids = np.random.choice(query_ids, size=50, replace=False)\n",
    "\n",
    "# Get all rows for those queries\n",
    "subset = filtered[filtered['query_id'].isin(sampled_qids)]\n",
    "print(f\"Rows with sampled queries: {len(subset):,}\")\n",
    "\n",
    "# Take 500 rows\n",
    "sample = subset.sample(n=min(500, len(subset)), random_state=42)\n",
    "\n",
    "print(f\"\\nFinal sample:\")\n",
    "print(f\"  Rows: {len(sample)}\")\n",
    "print(f\"  Queries: {sample['query_id'].nunique()}\")\n",
    "print(f\"  Products: {sample['product_id'].nunique()}\")\n",
    "\n",
    "# Quick peek at query diversity\n",
    "print(\"\\nSample queries:\")\n",
    "for q in sample['query'].drop_duplicates().head(5):\n",
    "    print(f\"  - {q}\")\n",
    "\n",
    "sample.to_csv(\"sample_dataset.csv\", index=False)\n",
    "print(\"\\n Saved to sample_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae770256-72d5-44e3-9070-c531e38d1141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b370d474-f057-4b8a-a9e6-2888b2da2f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products with text: 499\n",
      "Avg text length: 1363 chars\n",
      "\n",
      "Example product text:\n",
      "Minwax Wood Finish 227614444, Classic Gray Stain, Half Pint Minwax RICH EVEN COLOR – Minwax Wood Finish is a deep penetrating, oil-based wood stain that provides beautiful color and enhances the natur...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Product Text Preparation\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Key decision: Concatenate all product fields for richer representations\n",
    "- Title (most important)\n",
    "- Brand, color (important for filtering)\n",
    "- Bullets, description (detailed info)\n",
    "\n",
    "Based on the results in Production, I would also do some weighted sum -\n",
    "giving more importance to title, then brand, then product_bullet_point,\n",
    "then product_description.\n",
    "\"\"\"\n",
    "\n",
    "# Merge sample with products\n",
    "sample_keys = sample[['product_id', 'product_locale']].drop_duplicates()\n",
    "prod = sample_keys.merge(\n",
    "    products[['product_id', 'product_locale', 'product_title', 'product_brand', \n",
    "              'product_bullet_point', 'product_description']], \n",
    "    on=['product_id', 'product_locale'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Build combined text\n",
    "text_cols = ['product_title', 'product_brand', 'product_bullet_point', 'product_description']\n",
    "prod['product_text'] = prod[text_cols].fillna('').agg(' '.join, axis=1)\n",
    "prod['product_text'] = prod['product_text'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "# Check quality\n",
    "prod = prod[prod['product_text'].str.len() > 0].copy()\n",
    "print(f\"Products with text: {len(prod)}\")\n",
    "print(f\"Avg text length: {prod['product_text'].str.len().mean():.0f} chars\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample product text:\")\n",
    "print(prod.iloc[0]['product_text'][:200] + \"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36517b70-9525-4c73-9cf2-f1c1e29d0fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jahnavi Gajula\\.conda\\envs\\ml-dml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 384 dimensions\n",
      "Encoding products...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 4/4 [00:23<00:00,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built: 499 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Baseline - Dense Embeddings Only\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Approach 1: Pure semantic search with sentence transformers\n",
    "Model choice: all-MiniLM-L6-v2\n",
    "- Fast (384 dim)\n",
    "- Standard baseline\n",
    "Vector Database Choice: Using FAISS in-memory vector store since the sample data is only 500 rows.\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(f\"Model loaded: {model.get_sentence_embedding_dimension()} dimensions\")\n",
    "\n",
    "# Encode products\n",
    "prod_texts = prod['product_text'].tolist()\n",
    "prod_ids = (prod['product_id'].astype(str) + '||' + prod['product_locale'].astype(str)).tolist()\n",
    "\n",
    "print(\"Encoding products...\")\n",
    "embs = model.encode(prod_texts, batch_size=128, show_progress_bar=True, \n",
    "                   normalize_embeddings=True) #processes 128 products at a time\n",
    "\n",
    "# Build FAISS index\n",
    "index = faiss.IndexFlatIP(embs.shape[1])\n",
    "index.add(embs.astype('float32'))\n",
    "print(f\"Index built: {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f37e808-d919-4814-afe3-69cd09cbc33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 49 queries\n",
      "Avg relevant per query: 10.2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Evaluation Setup\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Ground truth: All sample rows are relevant (esci_label='E')\n",
    "Metrics: HITS@1, HITS@5, HITS@10, MRR\n",
    "\"\"\"\n",
    "\n",
    "# Build ground truth dict\n",
    "sample['_pid_key'] = (sample['product_id'].astype(str) + '||' + \n",
    "                      sample['product_locale'].astype(str))\n",
    "gt = sample.groupby('query_id').agg({\n",
    "    'query': 'first',\n",
    "    '_pid_key': list\n",
    "}).to_dict('index')\n",
    "\n",
    "print(f\"Ground truth: {len(gt)} queries\")\n",
    "print(f\"Avg relevant per query: {sample.groupby('query_id').size().mean():.1f}\")\n",
    "\n",
    "def hits_at_k(ranks, k):\n",
    "    return 1.0 if any(r < k for r in ranks) else 0.0\n",
    "\n",
    "def mrr(ranks):\n",
    "    return 0.0 if not ranks else 1.0 / (min(ranks) + 1)\n",
    "\n",
    "def eval_ranking(ranked_pids, relevant_pids):\n",
    "    ranks = [i for i, pid in enumerate(ranked_pids) if pid in relevant_pids]\n",
    "    return {\n",
    "        'hits@1': hits_at_k(ranks, 1),\n",
    "        'hits@5': hits_at_k(ranks, 5),\n",
    "        'hits@10': hits_at_k(ranks, 10),\n",
    "        'mrr': mrr(ranks)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a482c4a-30d5-4eac-b3f9-9aeaaafa03cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating baseline: 100%|█████████████████████████████████████████████████████████████| 49/49 [00:01<00:00, 33.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BASELINE: all-MiniLM-L6-v2 ===\n",
      "HITS@1:  0.980\n",
      "HITS@5:  1.000\n",
      "HITS@10: 1.000\n",
      "MRR:     0.990\n",
      "\n",
      "Worst queries (MRR):\n",
      "  0.50 - 'travel size fragrance spray'\n",
      "  1.00 - '20 inch storage bin'\n",
      "  1.00 - 'alpha chi omega'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Baseline Results\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Test pure embeddings approach\n",
    "\"\"\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "results_baseline = []\n",
    "\n",
    "for qid, data in tqdm(gt.items(), desc=\"Evaluating baseline\"):\n",
    "    q_emb = model.encode([data['query']], normalize_embeddings=True)\n",
    "    D, I = index.search(q_emb.astype('float32'), k=10)\n",
    "    \n",
    "    ranked = [prod_ids[i] for i in I[0]]\n",
    "    metrics = eval_ranking(ranked, set(data['_pid_key']))\n",
    "    \n",
    "    results_baseline.append({\n",
    "        'query_id': qid,\n",
    "        'query': data['query'],\n",
    "        **metrics\n",
    "    })\n",
    "\n",
    "df_baseline = pd.DataFrame(results_baseline)\n",
    "\n",
    "print(\"\\n=== BASELINE: all-MiniLM-L6-v2 ===\")\n",
    "print(f\"HITS@1:  {df_baseline['hits@1'].mean():.3f}\")\n",
    "print(f\"HITS@5:  {df_baseline['hits@5'].mean():.3f}\")\n",
    "print(f\"HITS@10: {df_baseline['hits@10'].mean():.3f}\")\n",
    "print(f\"MRR:     {df_baseline['mrr'].mean():.3f}\")\n",
    "\n",
    "# Find problem cases\n",
    "worst = df_baseline.nsmallest(3, 'mrr')\n",
    "print(f\"\\nWorst queries (MRR):\")\n",
    "for _, row in worst.iterrows():\n",
    "    print(f\"  {row['mrr']:.2f} - '{row['query']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0e29a0f-d158-46c8-943e-5821441b3994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5 model loaded: 384 dimensions\n",
      "\n",
      "Encoding products with E5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 4/4 [01:29<00:00, 22.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5 index built: 499 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating E5: 100%|███████████████████████████████████████████████████████████████████| 49/49 [00:03<00:00, 14.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Alternative Embedding Model - E5-Small-v2\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Experiment: Try E5-small-v2 (query-focused training)\n",
    "- Same 384 dimensions as MiniLM\n",
    "- Trained specifically on query-document matching\n",
    "- Instruction-based (requires 'query:' prefix)\n",
    "\n",
    "Decision point: Does query-specific training help?\n",
    "\"\"\"\n",
    "\n",
    "model_e5 = SentenceTransformer('intfloat/e5-small-v2')\n",
    "print(f\"E5 model loaded: {model_e5.get_sentence_embedding_dimension()} dimensions\")\n",
    "\n",
    "# E5 requires instruction prefixes\n",
    "print(\"\\nEncoding products with E5...\")\n",
    "prod_texts_e5 = [\"passage: \" + t for t in prod_texts]\n",
    "embs_e5 = model_e5.encode(prod_texts_e5, batch_size=128, show_progress_bar=True,\n",
    "                          normalize_embeddings=True)\n",
    "\n",
    "# Build E5 index\n",
    "index_e5 = faiss.IndexFlatIP(embs_e5.shape[1])\n",
    "index_e5.add(embs_e5.astype('float32'))\n",
    "print(f\"E5 index built: {index_e5.ntotal} vectors\")\n",
    "\n",
    "# Evaluate E5\n",
    "results_e5 = []\n",
    "for qid, data in tqdm(gt.items(), desc=\"Evaluating E5\"):\n",
    "    q_text_e5 = \"query: \" + data['query']\n",
    "    q_emb = model_e5.encode([q_text_e5], normalize_embeddings=True)\n",
    "    D, I = index_e5.search(q_emb.astype('float32'), k=10)\n",
    "    \n",
    "    ranked = [prod_ids[i] for i in I[0]]\n",
    "    metrics = eval_ranking(ranked, set(data['_pid_key']))\n",
    "    \n",
    "    results_e5.append({\n",
    "        'query_id': qid,\n",
    "        'query': data['query'],\n",
    "        **metrics\n",
    "    })\n",
    "\n",
    "df_e5 = pd.DataFrame(results_e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845d7ec4-6ab3-4658-b006-1a405789c59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E5-small-v2 vs MiniLM ===\n",
      "Metric         MiniLM         E5     Diff\n",
      "------------------------------------------\n",
      "HITS@1          0.980      1.000   +0.020\n",
      "HITS@5          1.000      1.000   +0.000\n",
      "HITS@10         1.000      1.000   +0.000\n",
      "MRR             0.990      1.000   +0.010\n"
     ]
    }
   ],
   "source": [
    "# Analysis of E5- small vs BERT mini\n",
    "print(\"\\n=== E5-small-v2 vs MiniLM ===\")\n",
    "print(f\"{'Metric':<10} {'MiniLM':>10} {'E5':>10} {'Diff':>8}\")\n",
    "print(\"-\" * 42)\n",
    "for metric in ['hits@1', 'hits@5', 'hits@10', 'mrr']:\n",
    "    mini = df_baseline[metric].mean()\n",
    "    e5 = df_e5[metric].mean()\n",
    "    print(f\"{metric.upper():<10} {mini:>10.3f} {e5:>10.3f} {e5-mini:>+8.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd16b27-83cd-409e-bdf3-7374f22317f1",
   "metadata": {},
   "source": [
    "#### E5 is performing better. However, since this is a POC on a small dataset, the same results may not hold for larger datasets or production pipelines. In such cases, we would likely need to incorporate reranking or other mechanisms. For now, I would like to continue experimenting with different ranking approaches. If I observed similar results in a production setting, I would stop here and proceed with E5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f80f58-1351-408b-b6e5-0c5d555793b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Query 105041 'travel size fragrance spray'\n",
      "Relevant products: 10\n",
      "\n",
      "MiniLM Top-3:\n",
      "  1. [MISS] Pink by Victoria's Secret Eau De Parfum Spray for Women\n",
      "  2. [HIT] Victoria's Secret Bombshell Body Mist 2.5oz Travel Size\n",
      "  3. [HIT] Guerlain Mon Guerlain Eau De Parfum Mini Spray for Wome\n",
      "\n",
      "E5 Top-3:\n",
      "  1. [HIT] Victoria's Secret Bombshell Body Mist 2.5oz Travel Size\n",
      "  2. [HIT] Inis the Energy of the Sea Travel Cologne Spray, 0.5 Fl\n",
      "  3. [HIT] Tocca Travel Fragrance Spray - Cleopatra - 0.68 oz\n"
     ]
    }
   ],
   "source": [
    "# Deep dive: How did E5 fix the problematic query?\n",
    "problem_qid = 105041  # \"travel size fragrance spray\"\n",
    "if problem_qid in gt:\n",
    "    problem_query = gt[problem_qid]['query']\n",
    "    relevant_pids = set(gt[problem_qid]['_pid_key'])\n",
    "    \n",
    "    print(f\"\\nQuery {problem_qid} '{problem_query}'\")\n",
    "    print(f\"Relevant products: {len(relevant_pids)}\")\n",
    "    \n",
    "    # MiniLM results\n",
    "    q_emb_mini = model.encode([problem_query], normalize_embeddings=True)\n",
    "    D_mini, I_mini = index.search(q_emb_mini.astype('float32'), k=10)\n",
    "    ranked_mini = [prod_ids[i] for i in I_mini[0]]\n",
    "    \n",
    "    # E5 results\n",
    "    q_emb_e5 = model_e5.encode([\"query: \" + problem_query], normalize_embeddings=True)\n",
    "    D_e5, I_e5 = index_e5.search(q_emb_e5.astype('float32'), k=10)\n",
    "    ranked_e5 = [prod_ids[i] for i in I_e5[0]]\n",
    "    \n",
    "    print(\"\\nMiniLM Top-3:\")\n",
    "    for i, pid in enumerate(ranked_mini[:3], 1):\n",
    "        mark = \"[HIT]\" if pid in relevant_pids else \"[MISS]\"\n",
    "        idx = prod_ids.index(pid)\n",
    "        title = prod.iloc[idx]['product_title'][:55]\n",
    "        print(f\"  {i}. {mark} {title}\")\n",
    "    \n",
    "    print(\"\\nE5 Top-3:\")\n",
    "    for i, pid in enumerate(ranked_e5[:3], 1):\n",
    "        mark = \"[HIT]\" if pid in relevant_pids else \"[MISS]\"\n",
    "        idx = prod_ids.index(pid)\n",
    "        title = prod.iloc[idx]['product_title'][:55]\n",
    "        print(f\"  {i}. {mark} {title}\")\n",
    "    \n",
    "    # Compute metrics for this query\n",
    "    mini_metrics = eval_ranking(ranked_mini, relevant_pids)\n",
    "    e5_metrics = eval_ranking(ranked_e5, relevant_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e851154-fe1b-41d4-b116-c8ec42c27572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since E5 is trained on large corpus of query-document this helped, but still want to explore further\n",
    "# For POC: Continue with MiniLM to show full experimental journey\n",
    "# In production: Would use E5\n",
    "model_final = model\n",
    "index_final = index\n",
    "df_best = df_baseline\n",
    "prefix_query = \"\"\n",
    "prefix_doc = \"\"\n",
    "model_name = \"all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b993e1e-584f-4a43-b142-c2d7aa81906d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing MiniLM failure: 'travel size fragrance spray'\n",
      "\n",
      "Relevant items: 10\n",
      "\n",
      "MiniLM Top 10:\n",
      " 1. [MISS] Pink by Victoria's Secret Eau De Parfum Spray for Women, 2.5\n",
      " 2. [HIT] Victoria's Secret Bombshell Body Mist 2.5oz Travel Size\n",
      " 3. [HIT] Guerlain Mon Guerlain Eau De Parfum Mini Spray for Women, 0.\n",
      " 4. [HIT] Tocca Travel Fragrance Spray - Cleopatra - 0.68 oz\n",
      " 5. [HIT] Emeraude Exclamation Cologne Body Spray by Emeraude 2.5 Flui\n",
      " 6. [HIT] Viktor & Rolf Flowerbomb 0.68 oz Eau de Parfum Spray Fragran\n",
      " 7. [HIT] Inis the Energy of the Sea Travel Cologne Spray, 0.5 Fluid O\n",
      " 8. [MISS] Victoria's Secret Love Pink Eau de Parfum Spray, 1.7 Ounce\n",
      " 9. [MISS] Victoria's Secret Pink Pink Gold Eau De Parfum 1 Ounce (30 M\n",
      "10. [HIT] Tocca Travel Spray Eau de Parfum, 20ml (Stella)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Root Cause Analysis (MiniLM Baseline)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Even though E5 fixed the problem, let's analyze WHY MiniLM failed\n",
    "This helps understand if hybrid approach would be even more robust\n",
    "\n",
    "Pure embeddings miss exact keyword matches\n",
    "Example: \"travel size fragrance spray\" - needs exact \"travel size\" match\n",
    "\"\"\"\n",
    "\n",
    "# Analyze the problematic MiniLM query we saw earlier\n",
    "qid = 105041  # travel size fragrance spray\n",
    "qtext = gt[qid]['query']\n",
    "\n",
    "print(f\"Analyzing MiniLM failure: '{qtext}'\")\n",
    "\n",
    "# MiniLM results\n",
    "q_emb = model.encode([qtext], normalize_embeddings=True)\n",
    "D, I = index.search(q_emb.astype('float32'), k=10)\n",
    "ranked = [prod_ids[i] for i in I[0]]\n",
    "\n",
    "relevant = set(gt[qid]['_pid_key'])\n",
    "print(f\"\\nRelevant items: {len(relevant)}\")\n",
    "print(\"\\nMiniLM Top 10:\")\n",
    "for i, pid in enumerate(ranked[:10]):\n",
    "    is_rel = \"[HIT]\" if pid in relevant else \"[MISS]\"\n",
    "    idx = prod_ids.index(pid)\n",
    "    title = prod.iloc[idx]['product_title'][:60]\n",
    "    print(f\"{i+1:2}. {is_rel} {title}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ae075-8bd2-43cd-a0c6-aa6b167132fc",
   "metadata": {},
   "source": [
    "Root Cause: Generic 'spray' and 'fragrance' match semantically but missing Exact 'travel size' keyword signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44fa663c-dbcc-4df7-8f9e-51b4686c0749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder loaded\n",
      "\n",
      "Reranked results for: 'travel size fragrance spray'\n",
      "Top 10:\n",
      " 1. [HIT] Tocca Travel Fragrance Spray - Cleopatra - 0.68 oz\n",
      " 2. [HIT] Inis the Energy of the Sea Travel Cologne Spray, 0.5 Fluid O\n",
      " 3. [HIT] Victoria's Secret Bombshell Body Mist 2.5oz Travel Size\n",
      " 4. [HIT] Tocca Travel Spray Eau de Parfum, 20ml (Stella)\n",
      " 5. [HIT] Guerlain Mon Guerlain Eau De Parfum Mini Spray for Women, 0.\n",
      " 6. [MISS] Pink by Victoria's Secret Eau De Parfum Spray for Women, 2.5\n",
      " 7. [HIT] Viktor & Rolf Flowerbomb 0.68 oz Eau de Parfum Spray Fragran\n",
      " 8. [HIT] Emeraude Exclamation Cologne Body Spray by Emeraude 2.5 Flui\n",
      " 9. [HIT] Body Fantasies Signature Fragrance Body Spray, Japanese Cher\n",
      "10. [MISS] Victoria's Secret Love Pink Eau de Parfum Spray, 1.7 Ounce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating rerank: 100%|███████████████████████████████████████████████████████████████| 49/49 [05:02<00:00,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cross-Encoder Reranking ===\n",
      "HITS@1: 1.000 (was 0.980)\n",
      "MRR:    1.000 (was 0.990)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Experiment - Cross-Encoder Reranking\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Attempted Solution 1: Add cross-encoder reranker\n",
    "Goal: Use more powerful model to rerank top-K candidates\n",
    "One thing to note here is added latency might not be worth marginal gains\n",
    "Cross encoder also provides the relevance scores - for each query document pair\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512)\n",
    "print(\"Cross-encoder loaded\")\n",
    "\n",
    "def search_with_rerank(qtext, k=50):\n",
    "    \"\"\"Two-stage: FAISS retrieve + CE rerank\"\"\"\n",
    "    # Stage 1: FAISS\n",
    "    q_emb = model_final.encode([prefix_query + qtext], normalize_embeddings=True)\n",
    "    _, I = index_final.search(q_emb.astype('float32'), k=k)\n",
    "    candidates = [prod_ids[i] for i in I[0]]\n",
    "    \n",
    "    # Stage 2: Cross-encoder\n",
    "    pairs = [(qtext, prod.iloc[prod_ids.index(pid)]['product_text']) \n",
    "             for pid in candidates]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Sort by CE score\n",
    "    return [candidates[i] for i in scores.argsort()[::-1]]\n",
    "\n",
    "# Test on problem query\n",
    "rerank_result = search_with_rerank(qtext)\n",
    "print(f\"\\nReranked results for: '{qtext}'\")\n",
    "print(\"Top 10:\")\n",
    "for i, pid in enumerate(rerank_result[:10]):\n",
    "    is_rel = \"[HIT]\" if pid in relevant else \"[MISS]\"\n",
    "    idx = prod_ids.index(pid)\n",
    "    title = prod.iloc[idx]['product_title'][:60]\n",
    "    print(f\"{i+1:2}. {is_rel} {title}\")\n",
    "\n",
    "# Quick eval\n",
    "results_rerank = []\n",
    "for qid, data in tqdm(gt.items(), desc=\"Evaluating rerank\"):\n",
    "    ranked = search_with_rerank(data['query'])\n",
    "    metrics = eval_ranking(ranked, set(data['_pid_key']))\n",
    "    results_rerank.append({'query_id': qid, **metrics})\n",
    "\n",
    "df_rerank = pd.DataFrame(results_rerank)\n",
    "\n",
    "print(f\"\\n=== Cross-Encoder Reranking ===\")\n",
    "print(f\"HITS@1: {df_rerank['hits@1'].mean():.3f} (was {df_best['hits@1'].mean():.3f})\")\n",
    "print(f\"MRR:    {df_rerank['mrr'].mean():.3f} (was {df_best['mrr'].mean():.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36b24227-81d8-4132-a2fa-05c4f2e477ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Improvement: 1.0% MRR gain\n"
     ]
    }
   ],
   "source": [
    "#Analysis\n",
    "print(f\"   Improvement: {(df_rerank['mrr'].mean() - df_best['mrr'].mean())*100:.1f}% MRR gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec72ad-ff26-415e-be90-e23ebf232e23",
   "metadata": {},
   "source": [
    "This reranking model has cause additional latency per query, Let' see if we can Try lighter hybrid approach first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcf45a4e-2957-4789-871f-2466d7e7adf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid (Embeddings + BM25) results for: 'travel size fragrance spray'\n",
      "Top 10:\n",
      " 1. [HIT] Tocca Travel Fragrance Spray - Cleopatra - 0.68 oz\n",
      " 2. [HIT] Guerlain Mon Guerlain Eau De Parfum Mini Spray for Women, 0.\n",
      " 3. [MISS] Pink by Victoria's Secret Eau De Parfum Spray for Women, 2.5\n",
      " 4. [HIT] Victoria's Secret Bombshell Body Mist 2.5oz Travel Size\n",
      " 5. [HIT] Inis the Energy of the Sea Travel Cologne Spray, 0.5 Fluid O\n",
      " 6. [HIT] Emeraude Exclamation Cologne Body Spray by Emeraude 2.5 Flui\n",
      " 7. [HIT] Tocca Travel Spray Eau de Parfum, 20ml (Stella)\n",
      " 8. [HIT] Viktor & Rolf Flowerbomb 0.68 oz Eau de Parfum Spray Fragran\n",
      " 9. [MISS] Victoria's Secret Love Pink Eau de Parfum Spray, 1.7 Ounce\n",
      "10. [HIT] Body Fantasies Signature Fragrance Body Spray, Japanese Cher\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Hybrid Approach - Embeddings + BM25\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Combine embeddings (semantic) + BM25 (strong lexical baseline)\n",
    "Use Reciprocal Rank Fusion (RRF)\n",
    "Reason: BM25 is a widely used, production-friendly sparse retriever\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def _simple_tokenize(text: str) -> List[str]:\n",
    "    # lightweight, fast tokenizer good enough for BM25\n",
    "    return re.findall(r\"[a-z0-9]+\", text.lower())\n",
    "\n",
    "# Build BM25 index (ensure prod_texts aligns with prod_ids)\n",
    "corpus_tokens = [_simple_tokenize(t) for t in prod_texts]\n",
    "bm25 = BM25Okapi(corpus_tokens, k1=1.5, b=0.75)\n",
    "\n",
    "def search_hybrid(qtext, k=100, rrf_k=60):\n",
    "    \"\"\"Hybrid: embeddings + BM25 with RRF fusion.\"\"\"\n",
    "    # Dense retrieval via FAISS\n",
    "    q_emb = model_final.encode([prefix_query + qtext], normalize_embeddings=True)\n",
    "    _, faiss_idx = index_final.search(q_emb.astype('float32'), k=k)\n",
    "    faiss_pids = [prod_ids[i] for i in faiss_idx[0]]\n",
    "\n",
    "    # Sparse retrieval via BM25\n",
    "    q_tokens = _simple_tokenize(qtext)\n",
    "    bm25_scores = bm25.get_scores(q_tokens)  # array aligned to corpus/prod_ids\n",
    "    bm25_idx = bm25_scores.argsort()[::-1][:k]\n",
    "    bm25_pids = [prod_ids[i] for i in bm25_idx]\n",
    "\n",
    "    # RRF fusion (higher is better)\n",
    "    rrf = {}\n",
    "    for rank, pid in enumerate(faiss_pids):\n",
    "        rrf[pid] = rrf.get(pid, 0.0) + 1.0 / (rrf_k + rank + 1)\n",
    "    for rank, pid in enumerate(bm25_pids):\n",
    "        rrf[pid] = rrf.get(pid, 0.0) + 1.0 / (rrf_k + rank + 1)\n",
    "\n",
    "    # Sort by fused score\n",
    "    fused = sorted(rrf.items(), key=lambda kv: -kv[1])\n",
    "    return [pid for pid, _ in fused]\n",
    "\n",
    "# Test on the same problem query\n",
    "hybrid_result = search_hybrid(qtext)\n",
    "print(f\"\\nHybrid (Embeddings + BM25) results for: '{qtext}'\")\n",
    "print(\"Top 10:\")\n",
    "for i, pid in enumerate(hybrid_result[:10]):\n",
    "    is_rel = \"[HIT]\" if pid in relevant else \"[MISS]\"\n",
    "    idx = prod_ids.index(pid)\n",
    "    title = prod.iloc[idx]['product_title'][:60]\n",
    "    print(f\"{i+1:2}. {is_rel} {title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bac06f-6494-4a0c-8c8f-0380e5843f88",
   "metadata": {},
   "source": [
    "In this case, I observed that “MISS” appears in 3rd place with the Hybrid setup (Embeddings + BM25 ) versus 6th place with the cross-encoder. This indicates that query quality is affected slightly. In a production setting, I would validate this more rigorously using ranking metrics such as NDCG (Normalized Discounted Cumulative Gain).\n",
    "\n",
    "That said, the final choice depends on the trade-off between speed and accuracy. For this use case, I believe speed is more critical, so I would lean toward adopting lightweight hybrid embeddings that provide faster inference while maintaining acceptable ranking quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e35e51f3-1368-4ab1-9803-4990884a6baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating hybrid: 100%|███████████████████████████████████████████████████████████████| 49/49 [00:02<00:00, 24.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL COMPARISON ===\n",
      "Metric       Baseline   Reranker     Hybrid\n",
      "------------------------------------------------------\n",
      "HITS@1          0.980      1.000      1.000\n",
      "HITS@5          1.000      1.000      1.000\n",
      "HITS@10         1.000      1.000      1.000\n",
      "MRR             0.990      1.000      1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: Hybrid Evaluation & Comparison\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Final comparison: Baseline vs Reranker vs Hybrid\n",
    "\"\"\"\n",
    "\n",
    "results_hybrid = []\n",
    "for qid, data in tqdm(gt.items(), desc=\"Evaluating hybrid\"):\n",
    "    ranked = search_hybrid(data['query'])\n",
    "    metrics = eval_ranking(ranked, set(data['_pid_key']))\n",
    "    results_hybrid.append({'query_id': qid, 'query': data['query'], **metrics})\n",
    "\n",
    "df_hybrid = pd.DataFrame(results_hybrid)\n",
    "\n",
    "print(\"\\n=== FINAL COMPARISON ===\")\n",
    "print(f\"{'Metric':<10} {'Baseline':>10} {'Reranker':>10} {'Hybrid':>10}\")\n",
    "print(\"-\" * 54)\n",
    "for metric in ['hits@1', 'hits@5', 'hits@10', 'mrr']:\n",
    "    base = df_best[metric].mean()\n",
    "    rerank = df_rerank[metric].mean()\n",
    "    hyb = df_hybrid[metric].mean()\n",
    "    print(f\"{metric.upper():<10} {base:>10.3f} {rerank:>10.3f} {hyb:>10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f0bbec1-a2b5-434a-8dde-cfc398c8bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# CELL 12: Production Insights & Next Steps\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Key Learnings & Production Recommendations\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Save all results\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "df_baseline.to_csv('artifacts/results_baseline.csv', index=False)\n",
    "df_e5.to_csv('artifacts/results_e5.csv', index=False)\n",
    "df_rerank.to_csv('artifacts/results_rerank.csv', index=False)\n",
    "df_hybrid.to_csv('artifacts/results_hybrid.csv', index=False)\n",
    "\n",
    "metrics = {\n",
    "    'baseline_minilm': {\n",
    "        'model': 'all-MiniLM-L6-v2',\n",
    "        'mrr': float(df_baseline['mrr'].mean()),\n",
    "        'hits@1': float(df_baseline['hits@1'].mean())\n",
    "    },\n",
    "    'baseline_e5': {\n",
    "        'model': 'e5-small-v2',\n",
    "        'mrr': float(df_e5['mrr'].mean()),\n",
    "        'hits@1': float(df_e5['hits@1'].mean()),\n",
    "        'note': 'Already perfect - query-specific training works'\n",
    "    },\n",
    "    'reranker_minilm': {\n",
    "        'approach': 'MiniLM + cross-encoder rerank',\n",
    "        'mrr': float(df_rerank['mrr'].mean()),\n",
    "        'hits@1': float(df_rerank['hits@1'].mean()),\n",
    "        'latency_overhead': '~50-100ms'\n",
    "    },\n",
    "    'hybrid_minilm': {\n",
    "        'approach': 'MiniLM + BM25 + RRF',\n",
    "        'mrr': float(df_hybrid['mrr'].mean()),\n",
    "        'hits@1': float(df_hybrid['hits@1'].mean()),\n",
    "        'latency_overhead': '~5ms'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76f0a2f1-a075-49e2-adf5-b82ab949591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KEY FINDINGS ===\n",
      "\n",
      "1. Model Selection:\n",
      "   MiniLM: 0.990 MRR, 0.980 HITS@1\n",
      "   E5:     1.000 MRR, 1.000 HITS@1\n",
      "   → E5's query-document training solved the baseline problem\n",
      "   → Diff MRR = +0.010\n",
      "\n",
      "2. Retrieval Enhancement (explored on MiniLM):\n",
      "   Pure embeddings:     0.980 HITS@1\n",
      "   + Cross-encoder:     1.000 HITS@1\n",
      "   + BM25 hybrid:     1.000 HITS@1\n"
     ]
    }
   ],
   "source": [
    "with open('artifacts/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"\\n=== KEY FINDINGS ===\")\n",
    "print(\"\\n1. Model Selection:\")\n",
    "print(f\"   MiniLM: {df_baseline['mrr'].mean():.3f} MRR, {df_baseline['hits@1'].mean():.3f} HITS@1\")\n",
    "print(f\"   E5:     {df_e5['mrr'].mean():.3f} MRR, {df_e5['hits@1'].mean():.3f} HITS@1\")\n",
    "print(f\"   → E5's query-document training solved the baseline problem\")\n",
    "print(f\"   → Diff MRR = +{(df_e5['mrr'].mean() - df_baseline['mrr'].mean()):.3f}\")\n",
    "\n",
    "print(\"\\n2. Retrieval Enhancement (explored on MiniLM):\")\n",
    "print(f\"   Pure embeddings:     {df_baseline['hits@1'].mean():.3f} HITS@1\")\n",
    "print(f\"   + Cross-encoder:     {df_rerank['hits@1'].mean():.3f} HITS@1\")\n",
    "print(f\"   + BM25 hybrid:     {df_hybrid['hits@1'].mean():.3f} HITS@1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523dca4e-f030-41bc-8f8b-864ec6e56d34",
   "metadata": {},
   "source": [
    "At the end, I would add logging to track latency and response times in real use. This would show me how much delay affects business outcomes compared to accuracy gains. If needed, I would run statistical tests to check if the differences are meaningful and then make decisions based on that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7d68c-741b-4629-b9e9-973981207e24",
   "metadata": {},
   "source": [
    "### NEXT STEPS (Production Considerations):\n",
    "\n",
    "Use OpenSearch with HNSW instead of just in-memory Faiss, since it’s production-ready, scalable, and widely adopted.\n",
    "\n",
    "Try out ColBERT v2 or similar late-interaction models, which are commonly used in real-world search deployments.\n",
    "\n",
    "Add query normalization (lowercasing, removing stopwords, spelling fixes, and dictionary-based mappings (abbreviations → full form)) to reduce noise and improve recall.\n",
    "\n",
    "Evaluate with per-query recall and error analysis to understand exactly where the system misses and why.\n",
    "\n",
    "Experiment with a weighted fusion of product fields (title > brand > bullet_point > description) since title usually carries the strongest signal.\n",
    "\n",
    "Continuously run production assessments to see which design choices actually improve user experience and business metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e8afc-d1de-4a75-ab76-8458b1550f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-dml)",
   "language": "python",
   "name": "ml-dml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
